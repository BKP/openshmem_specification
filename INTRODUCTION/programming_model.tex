\section{Programming Model Overview}
%SP: Addressing suggestions from discussion on 01/28/2014 Merging the commented portions into the body. 
%The \openshmem programming model consists of library functions that provide
%low-latency, high-bandwidth communication  for  use  in  highly  parallelized 
%scalable programs. The functions in the \openshmem \ac{API} provide a programming 
%model for exchanfging data between cooperating parallel processes. The resulting programs are similar 
%in style to \ac{MPI} programs. The \openshmem \ac{API} can be used either alone 
%or in combination with \ac{MPI} functions in the same parallel program.
%\openshmem implements a \ac{PGAS} model. 
In the \ac{PGAS} model, each process has a local and globally shared memory where portions of the shared memory may have affinity to a particular process. \openshmem implements \ac{PGAS} by defining symmetric data objects as a mechanism to share information among \openshmem processes or \acp{PE}. \openshmem is a library and unlike UPC, CAF, Titanium, X10 and Chapel, which are all PGAS languages, it relies on the programmer to use the library calls  to implement the correct semantics of its programming model.

The \openshmem library functions have the potential to provide low-latency, high-bandwidth communication \ac{API} for use in highly parallelized scalable programs. The \ac{API} allows communication and synchronization operations on both private (local) and remotely accessible data objects. The key feature of \openshmem is that data transfer functions are \textit{\textbf{one-sided}} in nature. This means that a local \ac{PE} executing a data transfer does not require the participation of the remote \ac{PE} to complete the operation. This allows for overlap between communication and computation to hide data transfer latencies, which makes  \openshmem ideal for unstructured, small/medium size data communication patterns.
%\rcomment{Manju: To do - Make sure the first paragraph does not say the same
%things has first paragraph of Section 1}

%An \openshmem program is currently \ac{SPMD} in style. The
%\openshmem  processes, called \ac{PE}s, all start at the
%same time, and they all run the same program. Usually the \ac{PE}s perform
%computation on their own subdomains of the larger problem, and periodically 
%communicate with other \ac{PE}s to exchange information on which the
%next computation phase depends.
%SP: Addressing suggestions from discussion on 01/31/2014

%Data latency is  the  period  of  time that starts when a \ac{PE} initiates a transfer of data 
%and ends when a \ac{PE} can use the data. %SP: What about put? Not guaranteed till synchronization is hit.

%SP: Addressing suggestions from discussion on 01/28/2014
%\openshmem functions support remote data transfer through \FUNC{put} operations, which  transfer data to a 
%different \ac{PE}, get operations, which transfer data from a different \ac{PE}, and remote pointers, which 
%allow direct  references  to  data objects owned by another \ac{PE}. Other operations supported are \FUNC{collective} 
%\FUNC{broadcast} and \FUNC{reduction}, \FUNC{barrier synchronization}, and \FUNC{atomic memory operations}. 
%An atomic memory operation  is an atomic read-and-update operation, such as a fetch-and-increment, on a remote
%or local data object.

%\rcomment{Manju: [The idea is to talk SPMD. We are talking about nature of interfaces rather than 
% the interfaces that enable SPMD. Replace the second paragraph with the one below ?]}
%\rcomment{\\ Oscar: I'm good with this change. minor change to say the SPMD can be used to decompose work\\}
The \openshmem{} interfaces can be used to implement \ac{SPMD} style programs. It provides interfaces 
to start the \openshmem{} \ac{PE}s in parallel, and communication and synchronization interfaces to access symmetric data objects across \ac{PE}s. These interfaces can be leveraged to divide a problem into multiple sub-problems that can solved independently or with coordination using the communication and synchronization interfaces.
The \openshmem specification defines library calls, constants, variables, and language bindings for \Clang{} and \Fortran{}.
The \Cpp{} interface is currently the same as that for \Clang. An overview of the important \openshmem operations is described below:

\begin{enumerate}
\item \textbf{One-sided Data Transfers }

\begin{enumerate}
\item \FUNC{Put}: The local \ac{PE} specifies the source
data (local or symmetric) that is copied to the symmetric data object on the remote \ac{PE}. 
\item \FUNC{Get}: The local \ac{PE} specifies the symmetric data object on the remote \ac{PE}
that is copied to a data object (local or symmetric) on the local \ac{PE}. 
\end{enumerate}

\item \textbf{Synchronization Mechanisms }
\begin{enumerate}
\item \FUNC{Fence}: The \ac{PE} calling fence ensure ordering of \FUNC{Put} operations with respect to a specific target \ac{PE}. 
\item \FUNC{Quiet}: The \ac{PE} calling quiet ensures ordering of \FUNC{Put} operations before the next local or remote update. 
\item \FUNC{Barrier}: All or some \ac{PE}s collectively synchronize and ensure completion of remote and local updates prior to any \ac{PE} returning from the call.
\end{enumerate}
\item \textbf{Collective Communication}

\begin{enumerate}
\item \FUNC{Broadcast}: The \textit{root} \ac{PE} specifics a symmetric data object to be copied to a symmetric data object on one or more remote \ac{PE}s (not including itself). 
\item \FUNC{Collection}: All \ac{PE}s participating in the operation get the result of concatenated symmetric objects contributed by each of the \ac{PE} in another symmetric data object.
\item \FUNC{Reduction}: All \ac{PE}s participating in the operation get the result of associative binary operation over elements of the specified symmetric data object on another symmetric data object. 
\end{enumerate}

\item \textbf{Managing symmetric data objects}
\begin{enumerate}
\item \FUNC{Allocation}: All executing \ac{PE}s must participate in the allocation of a symmetric data object with identical arguments.
\item  \FUNC{Deallocation}: All executing \ac{PE}s must participate in the deallocation of the same symmetric data object with identical arguments.
\item  \FUNC{Reallocation}: All executing \ac{PE}s must participate in the reallocation of the same symmetric data object with identical arguments.
\end{enumerate}

\item \textbf{Mutual Exclusion}
\begin{enumerate}
\item \FUNC{Set Lock}: The \ac{PE} acquires exclusive access to the region bounded by the symmetric \textit{lock} variable.
\item \FUNC{Test Lock}: The \ac{PE} tests the symmetric \textit{lock} variable for availability.
\item \FUNC{Clear Lock}: The \ac{PE} which has previously acquired the \textit{lock} releases it.
\end{enumerate}

\item \textbf{Atomic Memory Operations}
\begin{enumerate}
\item \FUNC{Swaps}: The \ac{PE} initiating the swap gets the old value of the symmetric data object it is copying a new value to on the remote \ac{PE}.
\item \FUNC{Increments}: The \ac{PE} initiating the increment adds 1 to the symmetric data object on the remote \ac{PE}.
\item \FUNC{Add}: The \ac{PE} initiating the add specifics the value to be added to the symmetric data object on the remote \ac{PE}.
\end{enumerate}

\item \textbf{Data Cache control \textit{(deprecated)}}
\begin{enumerate}
\item Implementation of mechanisms to exploit the capabilities of hardware
cache if available.
\end{enumerate}
\end{enumerate}

%\begin{description}
%\item [{{Note:}}] More information about \openshmem routines can be found
%in the Library Routines section.
%\end{description}
